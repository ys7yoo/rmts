
# ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ RMTS ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦â€ğŸ‘¦ 


[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![code](https://img.shields.io/badge/Code-Python3.9-blue)](https://docs.python.org/3/license.html)
[![data](https://img.shields.io/badge/Data-ASAP-green)](https://paperswithcode.com/dataset/asap)
[![data](https://img.shields.io/badge/Data-Feedback-red)](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/data)






## ğŸ“– Overview
![Image](https://github.com/user-attachments/assets/4c9b5615-2199-46a4-9893-cda0c243d2e3)
Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with fine-tuning-based essay scoring model using smaller large language models (S-LLMs). RMTS uses a LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays.

## ğŸ“‘ Paper
**Rationale Behind Essay Scores: Enhancing S-LLMâ€™s Multi-Trait Essay Scoring with Rationale Generated by LLMs**  
*Seong Yeub Chu, Jong Woo Kim, Mun Yong Yi*  
Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL '25). [`arXiv`](https://arxiv.org/abs/2410.14202)

## â­ Main Feature

### LLM-based trait-wise rationale generation system
- Rationale generation using LLM and rubric guideline.
![Image](https://github.com/user-attachments/assets/24487340-4328-4ece-9554-eef3420d0350)

### Essay scoring with smaller LLMs
- Incorporating a rationale with an essay for essay scoring using S-LLMs.


## ğŸ’» Getting Started


### Installation
```
scikit-learn            
scipy
tqdm
transformers==4.37.2
datasets
numpy
pandas
accelerate
```

### How to Run (Evaluator: GPT-3.5-Turbo / Dimension: Coherence)
<pre>
pip install -r requirements.txt
python main.py --model_name t5-base
</pre>

## ğŸ”§ Stack
- **Language**: Python
- **Utilized LLMs**: GPT-3.5-Turbo, Llama-3.1-8B-Instruct
- **Dependencies** : Refer to "requirements.txt"
- **Dataset** : ASAP & Feedback Prize


## Project Structure

<!-- ```markdown -->
<pre>
RMTS
â”œâ”€â”€data
â”‚   â”œâ”€â”€ essay
â”‚   â””â”€â”€ feedback
â”œâ”€â”€models
â”‚   â”œâ”€â”€ customized_modeling_bart
â”‚   â”œâ”€â”€ customized_modeling_led
â”‚   â”œâ”€â”€ customized_modeling_pegasus
â”‚   â”œâ”€â”€ customized_modeling_t5
â”œâ”€â”€evaluation
â””â”€â”€utils
</pre>


## <img width="24" height="24" src="https://img.icons8.com/emoji/48/llama-emoji.png" alt="llama-emoji"/> How to Run Think Aloud with LLama-3.1
<pre>
git lfs install
git clone https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
</pre>

